# ADR-019: STT タイムスタンプによる音声セグメント分割

## ステータス

Accepted（ハイブリッド方式に発展）

## コンテキスト

Gemini TTS のマルチスピーカーモードは、長い台本を一括送信すると生成が不安定になる。
対策として、話者ごとにシングルスピーカーモードで合成し、生成音声を台本行ごとに分割して元の順序に再アセンブルする方式を導入した。

音声の行分割に FFmpeg の `silencedetect` を使用していたが、文中の自然なポーズが文間のポーズと同等以上の長さになるケースがあり、カットポイントが誤検出される問題が発生した。
しきい値の調整では根本的に解決できないため、テキストと音声の対応関係に基づく正確な分割方式が必要になった。

## 決定

Google Cloud Speech-to-Text v2 API の単語レベルタイムスタンプ機能を使用し、生成済み音声を文字起こしして各単語の時刻情報を取得し、既知の台本テキストと照合して行境界を特定する。
さらに、STT で得た行境界を FFmpeg `silencedetect` の無音区間中間点にスナップするハイブリッド方式に発展させた。

## 方式の発展経緯

### Phase 1: silencedetect のみ

FFmpeg `silencedetect` で無音区間を検出し、無音区間の中間点でカットする方式。

- **問題**: 文中の句読点ポーズ（読点、感嘆符後など）が文間のポーズと同等の長さになるケースがあり、行内で誤分割が発生して行の順序が崩れる
- `ExpectedSegments` で無音区間を長さ順に絞る対策も試みたが、句読点ポーズの方が文間ポーズより長いケースがあり根本的に解決できなかった

### Phase 2: STT タイムスタンプのみ

STT で単語レベルのタイムスタンプを取得し、文字カウントベースで行境界を特定する方式に切り替えた。

- **改善**: テキストとの照合に基づくため、行の順序は正しくなった
- **問題**: STT の単語境界は必ずしも実際の無音区間と一致しない。単語間の中間点でカットするため、発話途中の音声が微妙に欠けたり残ったりして、カット品質（自然さ）が低い

### Phase 3: ハイブリッド方式（現行）

STT で「どこで切るか」（行境界の特定）を決め、silencedetect で「正確なカット位置」（無音区間の中間点）を決めるハイブリッド方式。

```
STT → AlignTextToTimestamps → DetectSilenceIntervals → SnapBoundariesToSilence → SplitPCMByTimestamps
```

- STT の行境界を、`maxSnapDistance`（500ms）以内の最寄り無音区間の中間点にスナップする
- 距離判定は無音区間の中間点ではなく**最寄りの端**までの距離を使用する（長い無音区間でも端が近ければスナップできるようにするため）
- 適切な無音区間が見つからなければ STT 境界をそのまま使用（フォールバック）

#### 距離判定を「端」にした理由

当初は無音区間の中間点までの距離で判定していたが、TTS が生成する文間ポーズは 500ms〜1s 程度の長い無音になることがある。
この場合、STT 境界が無音の開始付近にあっても中間点までの距離が `maxSnapDistance` を超えてしまい、スナップが発動しないケースがあった。
実際の運用で、STT 境界から無音開始点まで 317ms（十分近い）にもかかわらず、中間点まで 774ms のためスナップされず、発話途中でカットされる問題を確認した。

## 選択肢

### 選択肢 1: FFmpeg silencedetect による無音分割

- メリット: 外部サービス不要、実装がシンプル
- デメリット: 文中のポーズと文間のポーズを区別できず、誤分割が発生する

### 選択肢 2: Cloud Speech-to-Text タイムスタンプ

- メリット: テキストと音声の対応関係に基づくため正確、Go SDK で完結、既に GCP を使用しているためインフラ追加が不要
- デメリット: STT API の追加コスト、レイテンシが増加する。カット品質は silencedetect に劣る

### 選択肢 3: 強制アライメント（ctc-forced-aligner 等）

- メリット: TTS 音声に対して最高精度、GPU 不要（CTC 系）
- デメリット: Python 製のためマイクロサービス化が必要、運用コストが増加する

### 選択肢 4: 行ごとの個別合成

- メリット: 分割処理自体が不要
- デメリット: API レートリミット（10 req/min）に抵触する、文脈の連続性が失われる

### 選択肢 5: STT + silencedetect ハイブリッド（採用）

- メリット: STT の正確な行特定と silencedetect の高品質カットの両方の長所を得られる
- デメリット: STT と silencedetect の両方を実行するため処理時間が増加する

## 理由

- TTS で生成した音声はクリアで雑音がないため、STT の認識精度が非常に高い
- Go SDK（`cloud.google.com/go/speech/apiv2`）が提供されており、既存のバックエンドに自然に統合できる
- プロジェクトが既に GCP（Vertex AI, GCS, Cloud Tasks）を使用しているため、追加のインフラ構築が不要
- 強制アライメントは精度面では最良だが、Python マイクロサービスの運用コストが見合わない
- STT のみの方式ではカット品質に課題があり、silencedetect との組み合わせで解決できた

## パラメータ

| パラメータ | 値 | 説明 |
|-----------|-----|------|
| `noiseDB` | -30 | silencedetect の無音判定しきい値（dB） |
| `minSilenceSec` | 0.2 | silencedetect の最小無音継続時間（秒） |
| `maxSnapDistance` | 500ms | STT 境界から無音区間端までの最大スナップ距離 |

## 結果

- `internal/infrastructure/stt/` に Cloud STT クライアントを追加
- `internal/pkg/audio/align.go` にテキストアライメントロジックと `SnapBoundariesToSilence` を追加
- `internal/pkg/audio/split.go` に `DetectSilenceIntervals` を公開関数として追加
- `internal/service/audio_job.go` の再アセンブルロジックをハイブリッド方式に変更
- DI コンテナと環境変数設定を更新
- STT API 利用料金が音声生成ごとに発生する
- デバッグ用に分割前のスピーカー別音源を `tmp/audio-debug/{jobID}/` に WAV で保存
